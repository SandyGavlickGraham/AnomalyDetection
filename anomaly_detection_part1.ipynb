{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Exercises\nfile name: anomaly_detection.py or anomaly_detection.ipynb\n\n#### Discrete data + probability\nUse basic probability to identify anomalous request methods. You will want to make sure the text is normalized in order to reduce the noise.\n\n#### Time series + EMA\nDiscover users who are accessing our curriculum pages way beyond the end of their codeup time. What would the dataframe look like? Use time series method for detecting anomalies, like exponential moving average with %b.\n\n#### Clustering - DBSCAN\nUse dbscan to detect anomalies in other products from the customers dataset.\n\nUse dbscan to detect anomalies in number of bedrooms and finished square feet of property for the filtered dataset you used in the clustering project (single unit properties with a logerror)."},{"metadata":{},"cell_type":"markdown","source":"# %%%%%%%%%%%%%%%%%"},{"metadata":{},"cell_type":"markdown","source":"# Anomaly Detection of Discrete Data using Probability\n## Discrete data + probability\n\n### Use basic probability to identify anomalous request methods. You will want to make sure the text is normalized in order to reduce the noise."},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import division\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nfrom numpy import linspace, loadtxt, ones, convolve\nfrom sklearn.ensemble import IsolationForest\n\nimport numpy as np\nimport pandas as pd\nimport collections\nimport math\nfrom datetime import datetime, timedelta\n\nfrom sklearn import metrics\nfrom random import randint\nfrom matplotlib import style\n\nimport seaborn as sns\n\nstyle.use('fivethirtyeight')\n%matplotlib inline\n\npd.set_option('display.max_rows', 200_000)\n\nfrom ipaddress import ip_address\nimport re\nimport json\nfrom urllib.request import urlopen\n# import ipinfo  # worked in Jupyter Notebook, but not working in Kaggle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### User defined function to evaluate the models."},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(actual, predictions, output=True):\n    mse = metrics.mean_squared_error(actual, predictions)\n    rmse = math.sqrt(mse)\n\n    if output:\n        print('MSE:  {}'.format(mse))\n        print('RMSE: {}'.format(rmse))\n    else:\n        return mse, rmse    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### User defined function to plot the models."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_and_eval(predictions, actual, metric_fmt='{:.2f}', linewidth=4):\n    if type(predictions) is not list:\n        predictions = [predictions]\n\n    plt.figure(figsize=(16, 8))\n    plt.plot(train,label='Train')\n    plt.plot(test, label='Test')\n\n    for yhat in predictions:\n        mse, rmse = evaluate(actual, yhat, output=False)        \n        label = f'{yhat.name}'\n        if len(predictions) > 1:\n            label = f'{label} -- MSE: {metric_fmt} RMSE: {metric_fmt}'.format(mse, rmse)\n        plt.plot(yhat, label=label, linewidth=linewidth)\n\n    if len(predictions) == 1:\n        label = f'{label} -- MSE: {metric_fmt} RMSE: {metric_fmt}'.format(mse, rmse)\n        plt.title(label)\n\n    plt.legend(loc='best')\n    plt.show()    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Wrangle Data\n### Acquire"},{"metadata":{},"cell_type":"markdown","source":"sample data rows:\n\n2018-01-26 09:55:03 / 1 8 97.105.19.61\n\n2018-01-26 10:00:02 javascript-i/introduction/working-with-data-types-operators-and-variables 6 22 97.105.19.61\n\n2018-01-26 10:00:49 javascript-i/introduction/variables 6 22 97.105.19.61"},{"metadata":{},"cell_type":"markdown","source":"### Label each field from the curriculum log."},{"metadata":{"trusted":true},"cell_type":"code","source":"colnames=['access_date', 'access_time', 'page_viewed', 'user_id', 'cohort_id', 'ip']\n# df = pd.read_csv('./access.log',   # New data for Bayes cohort       \ndf = pd.read_csv('../input/anonymizedcurriculumaccesstxt/anonymized-curriculum-access.txt',          \n                 engine='python',\n                 header=None,\n                 index_col=False,\n                 names=colnames,\n                 sep=' ',\n#                  sep=r'\\s(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)(?![^\\[]*\\])',\n                 na_values='\"-\"',\n#                  usecols=[0, 3, 4, 5, 6, 7, 8]\n)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop rows that are missing cohort numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.cohort_id.fillna(0, inplace=True) ## had tried filling with 0... might come back and redo this...\ndf = df.dropna()\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check types."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Change types to desired types."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['cohort_id'] = df.cohort_id.astype('int')\ndf['user_id'] = df.user_id.astype('int')\n\ndf['access_date'] = pd.to_datetime(df.access_date)\ndf['access_time'] = df.access_time.astype('str')\ndf['page_viewed'] = df.page_viewed.astype('str')\ndf['full_datetime'] = df.access_date.astype('str') \\\n                 + ' ' \\\n                 + df.access_time.astype('str')\ndf['full_datetime'] = pd.to_datetime(df['full_datetime'])\n\ndf['ip'] = df.ip.astype('str')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head().append(df.tail())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's see what the date range is for these observations."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.access_date.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.access_date.max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Our data ranges from January 26, 2018, to April 2, 2019."},{"metadata":{},"cell_type":"markdown","source":"#### Let's examine what kind of values we have in the \"page_viewed\" column."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_colwidth = 100\ndf.page_viewed.unique().tolist()[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We'll leave those alone for now."},{"metadata":{},"cell_type":"markdown","source":"### Import cohort file"},{"metadata":{"trusted":true},"cell_type":"code","source":"colnames=['cohort_id','cohort_name','start_date','end_date']\ncohorts = pd.read_csv('../input/cohort-datacsv/cohort_data.csv', \n                      names=colnames,\n                      header=0)     \ncohorts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note that cohorts 1 through 20 ended before the earliest access date in our observations.\n\nCheck the types in df and cohorts to see if anything needs to be changes."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cohorts.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Change types in cohorts dataframe to match up with the df dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"cohorts['cohort_name'] = cohorts.cohort_name.astype('str')\ncohorts['start_date']= pd.to_datetime(cohorts.start_date)\ncohorts['end_date'] = pd.to_datetime(cohorts.end_date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cohorts.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merge the cohort information onto the df dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.merge(cohorts, on='cohort_id', how='left')\ndf.shape # (219070, 6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's try grouping the observations only by cohort_id."},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_by_cohort = df.groupby('cohort_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"minimums = grouped_by_cohort.access_date.min()\nmaximums = grouped_by_cohort.access_date.max()\nmin_max_dates = pd.DataFrame()\nmin_max_dates['cohort_id'] = minimums.index\nmin_max_dates['earliest_access_date_for_cohort'] = minimums\nmin_max_dates['latest_access_date_for_cohort'] = maximums\nmin_max_dates['diff'] = maximums - minimums\nmin_max_dates","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note the no one in cohorts 1, 6, 8, 14, or 25 accessed the currilum during the timespan of our log, so let's drop those rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"min_max_dates.dropna(inplace=True)\nmin_max_dates.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Check the types and then merge."},{"metadata":{"trusted":true},"cell_type":"code","source":"min_max_dates.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.merge(min_max_dates, on=\"cohort_id\")\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### If the access happened after the class ended, days_after_end will be positive.\n#### If the access happened before the class ended, days_after_end will be negative."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['days_after_end'] =  df.access_date - df.end_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['cohort_name', 'user_id', 'access_date', 'end_date', 'days_after_end']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.cohort_name.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(['cohort_name'])['days_after_end'].max().sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Discover users who are accessing our curriculum pages way beyond the end of their codeup time. What would the dataframe look like? Use time series method for detecting anomalies, like exponential moving average with %b."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(['cohort_name', 'end_date', 'access_date', 'user_id'])['page_viewed'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.cohort_id.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.days_after_end > timedelta(days=134)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"late_access = df[df.days_after_end > timedelta(days=134)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"late_access.groupby('user_id')['access_date'].count().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_late_access = late_access.groupby('user_id')['access_date'].count().sort_values(ascending=False)\nlate_users = max_late_access[:5]\nlate_users","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The users with user_ids 11, 314, 1, 68, and 64 were the users who accessed the curriculum over 1000 times each and over 134 days after their cohorts ended."},{"metadata":{},"cell_type":"markdown","source":"# The following is a list of their accesses."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_late_user_df = df[df['user_id'].isin([11, 314, 1, 68, 64])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_late_user_df.sort_values(by='user_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}